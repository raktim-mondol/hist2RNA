# hist2scRNA: State-of-the-Art Single-Cell RNA-seq Prediction from Histopathology Images

## Overview

**hist2scRNA** is a cutting-edge deep learning framework for predicting single-cell RNA sequencing (scRNA-seq) expression profiles directly from histopathology images. This extends the original hist2RNA project from bulk RNA prediction to spatially-resolved single-cell level predictions.

## Key Features

- **Vision Transformer (ViT)** architecture for superior feature extraction from H&E images
- **Graph Neural Networks (GNN)** for modeling spatial relationships between cells/spots
- **Zero-Inflated Negative Binomial (ZINB) loss** to properly handle the sparsity characteristic of single-cell data
- **Multi-task learning** with simultaneous gene expression and cell type prediction
- **Spatial attention mechanisms** for capturing tissue architecture
- **State-of-the-art performance** based on recent methods (GHIST, Hist2ST, TransformerST)

## Architecture

ðŸ“Š **Visual Diagrams:** See [ARCHITECTURE_DIAGRAMS.md](ARCHITECTURE_DIAGRAMS.md) for comprehensive Mermaid diagrams of the architecture, data flow, and training process.

The hist2scRNA model consists of three main components:

### 1. Vision Transformer (ViT) Encoder
- Processes histopathology image patches (default: 224Ã—224 pixels)
- Divides images into smaller patches (default: 16Ã—16 pixels)
- Uses multi-head self-attention to capture long-range dependencies
- Generates rich feature representations for each spatial location

### 2. Spatial Graph Attention Network
- Models spatial relationships between neighboring spots/cells
- Uses Graph Attention Networks (GAT) to aggregate information from nearby locations
- Captures tissue microenvironment effects on gene expression
- Learns which neighbors are most relevant for prediction

### 3. Gene Expression Decoder
- Predicts parameters of Zero-Inflated Negative Binomial distribution:
  - **Î¼ (mu)**: Mean expression level for each gene
  - **Î¸ (theta)**: Dispersion parameter capturing gene-specific variability
  - **Ï€ (pi)**: Zero-inflation probability modeling dropout events
- Simultaneously predicts cell type for each spot (multi-task learning)

## Model Variants

### Full Model (`hist2scRNA`)
- Complete Vision Transformer with 6-12 layers
- Full spatial graph attention
- ~50-100M parameters
- Best performance for research applications

### Lightweight Model (`hist2scRNA_Lightweight`)
- Simplified architecture using pre-extracted features
- Efficient spatial attention without full ViT
- ~10-20M parameters
- Faster training and inference for production use

## Installation

```bash
# Clone the repository
git clone https://github.com/raktim-mondol/hist2RNA.git
cd hist2RNA

# Install dependencies
pip install -r requirements.txt

# Install PyTorch Geometric (for GNN components)
pip install torch_geometric
```

## Quick Start

### 1. Generate Dummy Data (for testing)

```bash
python generate_dummy_scrna_data.py
```

This will create:
- `dummy_data/small/` - Small dataset (50 spots, 500 genes) for quick testing
- `dummy_data/medium/` - Medium dataset (200 spots, 2000 genes) for realistic testing

Each dataset includes:
- Synthetic H&E histopathology patches
- Simulated single-cell gene expression with realistic sparsity
- Spatial coordinates and graph structure
- Cell type labels
- Visualizations

### 2. Test Model Architecture

```bash
python test_scrna_model.py
```

This verifies that the model architecture is working correctly without requiring training.

### 3. Train the Model

```bash
# Train on small dataset (quick test)
python train_hist2scRNA.py \
    --data_dir ./dummy_data/small \
    --epochs 50 \
    --batch_size 8 \
    --model_type full \
    --output_dir ./output_scrna_small

# Train on medium dataset (realistic)
python train_hist2scRNA.py \
    --data_dir ./dummy_data/medium \
    --epochs 100 \
    --batch_size 4 \
    --model_type full \
    --lr 0.0001 \
    --output_dir ./output_scrna_medium
```

### 4. Training with Custom Parameters

```bash
python train_hist2scRNA.py \
    --data_dir /path/to/your/data \
    --model_type full \
    --img_size 224 \
    --patch_size 16 \
    --embed_dim 384 \
    --vit_depth 6 \
    --vit_heads 6 \
    --epochs 100 \
    --batch_size 8 \
    --lr 0.0001 \
    --weight_decay 0.01 \
    --alpha 0.1 \
    --output_dir ./output_scrna \
    --checkpoint_path ./output_scrna/best_model.pt
```

## Data Format

The model expects the following data structure:

```
data_dir/
â”œâ”€â”€ patches/                    # Histopathology image patches
â”‚   â”œâ”€â”€ spot_0000.png
â”‚   â”œâ”€â”€ spot_0001.png
â”‚   â””â”€â”€ ...
â”œâ”€â”€ gene_expression.csv         # Gene expression matrix (spots Ã— genes)
â”œâ”€â”€ spatial_coordinates.csv     # Spatial coordinates (spot_id, x, y)
â”œâ”€â”€ spatial_edges.csv          # Graph edges (source, target)
â”œâ”€â”€ cell_types.csv             # Cell type labels (spot_id, cell_type)
â””â”€â”€ metadata.json              # Dataset metadata
```

### File Formats

#### gene_expression.csv
```csv
spot_id,Gene_0000,Gene_0001,Gene_0002,...
spot_0000,5.2,0.0,12.3,...
spot_0001,3.1,8.7,0.0,...
```

#### spatial_coordinates.csv
```csv
spot_id,x,y
spot_0000,1.5,2.3
spot_0001,1.7,2.5
```

#### spatial_edges.csv
```csv
source,target
0,1
0,2
1,3
```

#### cell_types.csv
```csv
spot_id,cell_type
spot_0000,0
spot_0001,1
```

## Model Parameters

### Architecture Parameters
- `img_size`: Input image size (default: 224)
- `patch_size`: Patch size for ViT (default: 16)
- `embed_dim`: Embedding dimension (default: 384)
- `vit_depth`: Number of transformer layers (default: 6)
- `vit_heads`: Number of attention heads (default: 6)
- `dropout`: Dropout rate (default: 0.1)

### Training Parameters
- `epochs`: Number of training epochs (default: 50)
- `batch_size`: Batch size (default: 8)
- `lr`: Learning rate (default: 0.0001)
- `weight_decay`: Weight decay for AdamW (default: 0.01)
- `alpha`: Weight for cell type classification loss (default: 0.1)

## Evaluation Metrics

The model is evaluated using multiple metrics:

1. **Gene Expression Metrics**
   - Mean Squared Error (MSE)
   - Mean Absolute Error (MAE)
   - Mean Spearman correlation per spot
   - Mean Spearman correlation per gene

2. **Cell Type Classification**
   - Accuracy
   - Per-class precision/recall (optional)

3. **Loss Components**
   - ZINB loss (gene expression)
   - Cross-entropy loss (cell type)
   - Total combined loss

## Advanced Usage

### Using Pre-extracted Features

If you have pre-extracted features from a foundation model (e.g., UNI, Virchow):

```python
from hist2scRNA_model import hist2scRNA_Lightweight

model = hist2scRNA_Lightweight(
    feature_dim=2048,  # Dimension of pre-extracted features
    n_genes=2000,
    n_cell_types=10
)

# Train with pre-extracted features instead of images
features = torch.randn(batch_size, 2048)
output = model(features)
```

### Custom Loss Functions

You can modify the ZINB loss or add custom losses:

```python
from hist2scRNA_model import ZINBLoss
import torch.nn as nn

# Combine ZINB with additional regularization
criterion_zinb = ZINBLoss()
criterion_ce = nn.CrossEntropyLoss()

# Custom loss with additional spatial smoothness term
def custom_loss(output, target, spatial_graph):
    zinb_loss = criterion_zinb(output['mu'], output['theta'], output['pi'], target)
    ce_loss = criterion_ce(output['cell_type_logits'], cell_types)

    # Add spatial smoothness regularization
    # (expressions of neighboring spots should be similar)
    spatial_loss = compute_spatial_smoothness(output['mu'], spatial_graph)

    return zinb_loss + 0.1 * ce_loss + 0.01 * spatial_loss
```

## Theoretical Background

### Why Zero-Inflated Negative Binomial (ZINB)?

Single-cell RNA-seq data has two key characteristics:

1. **Sparsity (Zero-inflation)**: Many genes show zero expression due to:
   - Biological absence (gene truly not expressed)
   - Technical dropout (RNA molecule not captured)
   - ZINB models both sources with parameter Ï€

2. **Overdispersion**: Expression variance exceeds the mean
   - Negative Binomial captures this with dispersion parameter Î¸
   - Better than Poisson or Gaussian distributions

### Why Vision Transformers?

Recent research shows ViTs outperform CNNs for histopathology because:
- Self-attention captures long-range tissue architecture
- Better modeling of global context (tumor microenvironment)
- Patch-based processing natural for tissue organization
- Pre-training on large histopathology datasets available

### Why Graph Neural Networks?

Spatial transcriptomics data has inherent graph structure:
- Spots/cells are nodes with spatial coordinates
- Edges connect spatially proximal spots
- Gene expression is influenced by neighboring cells
- GAT learns which neighbors matter most for each spot

## Comparison with Other Methods

| Method | Architecture | Spatial Modeling | Loss Function | Year |
|--------|-------------|------------------|---------------|------|
| ST-Net | CNN | None | MSE | 2020 |
| Hist2ST | CNN + Transformer + GNN | Graph | ZINB | 2022 |
| HisToGene | ViT | Positional encoding | MSE | 2023 |
| TransformerST | ViT | Positional encoding | MSE | 2024 |
| GHIST | ViT + scRNA integration | Attention | Custom | 2025 |
| **hist2scRNA** | **ViT + GAT** | **Graph + Attention** | **ZINB + Multi-task** | **2025** |

## Performance Tips

1. **For faster training:**
   - Use `hist2scRNA_Lightweight` model
   - Reduce `embed_dim` and `vit_depth`
   - Increase `batch_size` if GPU memory allows
   - Use pre-extracted features

2. **For better performance:**
   - Use full `hist2scRNA` model
   - Increase `vit_depth` to 8-12
   - Use larger `embed_dim` (512-768)
   - Train longer (100-200 epochs)
   - Use data augmentation

3. **For limited GPU memory:**
   - Reduce `batch_size`
   - Use gradient accumulation
   - Reduce `embed_dim`
   - Use mixed precision training (fp16)

## Citation

If you use hist2scRNA in your research, please cite:

```bibtex
@Article{cancers15092569,
  AUTHOR = {Mondol, Raktim Kumar and Millar, Ewan K. A. and Graham, Peter H. and Browne, Lois and Sowmya, Arcot and Meijering, Erik},
  TITLE = {hist2RNA: An Efficient Deep Learning Architecture to Predict Gene Expression from Breast Cancer Histopathology Images},
  JOURNAL = {Cancers},
  VOLUME = {15},
  YEAR = {2023},
  NUMBER = {9},
  ARTICLE-NUMBER = {2569},
  URL = {https://www.mdpi.com/2072-6694/15/9/2569},
  DOI = {10.3390/cancers15092569}
}
```

And relevant papers for the techniques used:
- **GHIST**: He et al., "Spatial gene expression at single-cell resolution from histology using deep learning with GHIST", Nature Methods (2025)
- **Hist2ST**: Zeng et al., "Spatial transcriptomics prediction from histology jointly through Transformer and graph neural networks", Briefings in Bioinformatics (2022)
- **Vision Transformers**: Dosovitskiy et al., "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale", ICLR (2021)

## Contributing

We welcome contributions! Please see [CONTRIBUTING.md](CONTRIBUTING.md) for guidelines.

## License

This project is licensed under the MIT License - see [LICENSE](LICENSE) for details.

## Acknowledgments

This work builds upon:
- Original hist2RNA framework for bulk RNA prediction
- Recent advances in spatial transcriptomics prediction (GHIST, Hist2ST, TransformerST)
- Vision Transformer architectures for computational pathology
- Graph neural networks for spatial modeling

## Support

For questions and issues:
- Open an issue on GitHub
- Check existing documentation
- Contact: [raktim.mondol@example.com](mailto:raktim.mondol@example.com)

## Roadmap

Future enhancements:
- [ ] Integration with foundation models (UNI, Virchow, CONCH)
- [ ] Support for multi-modal data (H&E + IHC + IF)
- [ ] Attention visualization and interpretability
- [ ] Pre-trained models on public datasets
- [ ] Benchmark on 10X Visium and other platforms
- [ ] Super-resolution gene expression prediction
- [ ] Integration with cell segmentation methods
